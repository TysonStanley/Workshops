<!DOCTYPE html>
<html>
  <head>
    <title>Lasso and Ridge Regression</title>
    <meta charset="utf-8">
    <meta name="author" content="Tyson S. Barrett" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link rel="stylesheet" href="pres.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Lasso and Ridge Regression
## ðŸ¤—
### Tyson S. Barrett

---



class: inverse, center, middle

# When is regression great?
# When is it *not* so great?

---
class: center

.pull-left[
# ðŸ¤—

.coral[
### Interpretable
### Simple
### Flexible
]
]

--

.pull-right[
# ðŸ˜­

.coral[
### Correlated Predictors
### Many (but sparse) predictors
]
]

---
class: center

.pull-left[
# ðŸ¤—

.coral[
### Interpretable
### Simple
### Flexible
]
]

.pull-right[
# ~~ðŸ˜­~~ ðŸ¤—

.coral[
### Correlated Predictors
### Many (but sparse) predictors
]
]

&lt;br&gt;
&lt;br&gt;

.huge[**Regularized Regression** can help]

---

# Quick Review of Regression

.pull-left[
$$
Y = \beta_0 + \sum_j^p \beta_j X_j + e_i
$$

The estimated effect of `\(X_1\)` is:
$$
\frac{\delta Y}{\delta X_1} = \beta_1
$$
]

.pull-right[

#### It finds the best fitting line to the data.
#### It can handle categorical and continuous predictors and outcomes.
#### It has decent predictive accuracy (there are better methods).
]

&lt;br&gt;
&lt;br&gt;

.center[.large[.coral[But it struggles when predictors are highly correlated and when predictor selection is necessary.]]]

---
# Regularized Regression

.pull-left[
### Built on conventional regression but includes a penalty:

$$
Y = \beta_0 + \sum_j^p \beta_j X_j + \lambda f(\beta_j) + e_i
$$
where `\(f(\beta_j)\)` is a penalty function (i.e., if the `\(\beta_j\)` is larger, the penalty is larger) and `\(\lambda\)` is a tuning parameter (i.e., controls how strong the penalty is).
]

.pull-right[
### Why is this cool??
.coral[
#### It minimizes the residuals and the penalty simultaneously.
#### It gives it some magical powers.
#### It has better predictive accuracy than conventional regression.
#### Its usage is very similar to conventional regression.
]]

&lt;br&gt;

.center[.large[We'll discuss all of these in regards to specific techniques]]

&lt;br&gt;

.center[.large[But first, some general steps to using regularized regressions]]

---
background-image: url(LassoRidge_Steps.pdf)
background-size: contain
background-repeat: no-repeat

---
class: inverse, center, middle

# Popular Regularized Regressions

---
background-image: url(LassoRidge_Venn.pdf)
background-size: contain
background-repeat: no-repeat

---
# Lasso Regression

.pull-left[
### Built on conventional regression but includes a penalty:

$$
Y = \beta_0 + \sum_j^p \beta_j X_j + \lambda|\beta_j| + e_i
$$

The penalty is `\(|\beta_j|\)` -- the absolute value of `\(\beta_j\)`.
]

.pull-right[
### This penalty makes it so:
.coral[
#### The most important variables are selected (all other have estimates equal to zero)
#### It can handle more predictors than variables (p &gt; n).
#### It has better predictive accuracy than conventional regression. This means it is less "over-fit" and more generalizable.
#### Note: it struggles with correlated predictors.
]]

&lt;br&gt;

.center[.large[How do we use it?]]


---
# How do we use it?

.large[Can be done using `R` with `glmnet` or `SAS` with `proc glmselect`]

&lt;br&gt;

.large[I'll show it in `R` (at the end of the workshop, I'll walk through each aspect of the code)]

&lt;br&gt;
&lt;br&gt;

.large[We'll use a fake data set about two popular TV shows--The Office and Parks and Recreation.]

&lt;br&gt;

.coral[.large[Note: We'll be ignoring some assumptions (like the fact the data are nested).]]

---
# Dataset

<div id="htmlwidget-91611ed88bc47c60295c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-91611ed88bc47c60295c">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38"],["Michael","Pam","Jim","Dwight","Stanley","Phyllis","Creed","Meredith","Oscar","Angela","Kevin","Kelley","Ryan","Toby","Andy","Jan","April","Andy","Leslie","Ron","Tom","Donna","Ben","Chris","Gary (Larry, Jerry)","Jean Ralphio","Mona Lisa","Ann","Kyle","The Other Ron","Craig","Shauna Molwaytweep","Councilman Jamm","Ethel","Councilman Howser","Tammy","Tammy II","Diane"],[2,3,3,5,4,4,1,3,5,4,2,3,2,4,3,4,1,1,5,3,2,2,5,4,3,1,1,5,3,null,null,4,null,2,5,null,5,null],[3,8,8,6,7,8,2,5,7,5,6,5,2,1,5,6,6,2,8,8,5,7,8,6,5,1,1,8,5,null,null,6,null,5,6,null,5,null],[8,7,8,8,4,4,4,4,7,7,2,5,5,6,7,6,4,2,7,7,5,6,5,8,3,2,1,8,2,7,6,5,5,2,6,5,3,7],[0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,null,0,0,1],[0,1,0,0,0,1,0,1,0,1,0,1,0,0,0,1,1,0,1,0,0,1,0,0,0,0,1,1,1,0,1,0,1,0,1,0,0,0],["White","White","White","White","Black","White","White","White","Mexican American","White","White","Indian","White","White","White","White","Mexican American","White","White","White","Indian","Black","White","White","White","White","White","White","White","White","White","White","White","White","Black","White","White","White"],[55,35,70,70,70,70,45,40,50,50,45,40,40,60,60,80,25,15,45,55,35,70,65,70,40,10,10,40,35,35,35,45,90,40,60,55,40,90],[0,2,2,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,2],[1,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,1,1,1,1,1,1,1,1,1,1,1],[1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>nam<\/th>\n      <th>prod1<\/th>\n      <th>ment1<\/th>\n      <th>phys<\/th>\n      <th>marr<\/th>\n      <th>gend<\/th>\n      <th>race<\/th>\n      <th>inco<\/th>\n      <th>chil<\/th>\n      <th>subs<\/th>\n      <th>alco<\/th>\n      <th>spor<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[2,3,4,5,6,8,9,10,11,12]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---
# How do we use it?


```r
## The R Package
library(glmnet)

## Remove Missingness (or impute)
df &lt;- df %&gt;%
  filter(complete.cases(prod1, depr1, marr, chil))

## Outcome
Y &lt;- df$prod2

## Dummy Code
X &lt;- df %&gt;%
  select(-prod2, -nam, -start_date, -last_aired, -show)
X &lt;- model.matrix(~ ., X)[,-1]

fit &lt;- cv.glmnet(X, Y, alpha = 1)
plot(fit)
coef(fit, s = "lambda.min")
coef(fit, s = "lambda.1se")
```

---
# How do we use it?

.pull-left[
![](LassoRidge_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;

]

.pull-right[

```
##                Variable Lambda.Min Lambda.1SE
## 1           (Intercept)    1.76634    1.99227
## 2                 prod1    0.87933    0.73844
## 3                 ment1          -          -
## 4                  phys          -          -
## 5                  marr    -0.3087          -
## 6                  gend   -0.82335   -0.44721
## 7             raceBlack          -          -
## 8  raceMexican American          -          -
## 9            raceIndian          -          -
## 10                 inco          -          -
## 11                 chil          -          -
## 12                 subs          -          -
## 13                 alco          -          -
## 14                 spor          -          -
## 15                depr1          -          -
## 16                awkw1          -          -
## 17                ment2          -          -
## 18                depr2          -          -
## 19                awkw2          -          -
```
]

---
background-image: url(LassoRidge_Venn.pdf)
background-size: contain
background-repeat: no-repeat

---
# Ridge Regression

.pull-left[
### Built on conventional regression but includes a penalty:

$$
Y = \beta_0 + \sum_j^p \beta_j X_j + \lambda(\beta_j)^2 + e_i
$$

The penalty is `\((\beta_j)^2\)` -- the squared value of `\(\beta_j\)`.
]

.pull-right[
### This penalty makes it so:
.coral[
#### It can handle correlated predictors (even perfectly correlated)
#### It can handle far more predictors than variables (p &gt;&gt; n).
#### It has better predictive accuracy than conventional regression. This means it is less "over-fit" and more generalizable.
#### Note: it does not actually select variables (although the estimates can get very close to zero)
]]


.center[.large[How do we use it?]]

---
# How do we use it?

Just like Lasso but with `alpha = 0` (this will make more sense a bit later)

```r
## The R Package
library(glmnet)

## Remove Missingness (or impute)
df &lt;- df %&gt;%
  filter(complete.cases(prod1, depr1, marr, chil))

## Outcome
Y &lt;- df$prod2

## Dummy Code
X &lt;- df %&gt;%
  select(-prod2, -nam, -start_date, -last_aired, -show)
X &lt;- model.matrix(~ ., X)[,-1]

fit &lt;- cv.glmnet(X, Y, alpha = 0)
plot(fit)
coef(fit, s = "lambda.min")
coef(fit, s = "lambda.1se")
```

---
# How do we use it?

.pull-left[
![](LassoRidge_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

]

.pull-right[

```
##                Variable Lambda.Min Lambda.1SE
## 1           (Intercept)     3.0088    2.90129
## 2                 prod1    0.94508    0.50769
## 3                 ment1   -0.08598   -0.00496
## 4                  phys    0.10796    0.09214
## 5                  marr   -0.62115   -0.59835
## 6                  gend   -1.23115   -0.81477
## 7             raceBlack   -0.70391   -0.81241
## 8  raceMexican American   -0.28312    -0.0342
## 9            raceIndian   -0.18325   -0.18884
## 10                 inco   -0.02821   -0.00649
## 11                 chil    0.43105    0.18603
## 12                 subs    0.28644   -0.25424
## 13                 alco   -1.37837   -0.66774
## 14                 spor   -0.49028   -0.44431
## 15                depr1    0.08676    0.04819
## 16                awkw1   -0.01242   -0.01332
## 17                ment2    0.17527    0.09927
## 18                depr2   -0.02355   -0.00863
## 19                awkw2   -0.01758   -0.01108
```
]


---
background-image: url(LassoRidge_Venn.pdf)
background-size: contain
background-repeat: no-repeat

---
# Elastic Net

.pull-left[
#### It is the combination of Lasso and Ridge Regressions.

$$
Y = \beta_0 + \sum_j^p \beta_j X_j + \lambda\big[\alpha |\beta_j| + (1- \alpha) (\beta_j)^2\big] + e_i
$$

Note that it has a new tuning parameter, `\(\alpha\)`. Where have we seen that before?
]

.pull-right[
### This penalty makes it so:
.coral[
#### It can handle correlated predictors (even perfectly correlated)
#### It can handle far more predictors than variables (p &gt;&gt; n).
#### It has better predictive accuracy than conventional regression. This means it is less "over-fit" and more generalizable.
#### It selects the most important variables.
#### I.e. It combines the strengths of Lasso and Ridge.
]]


.center[.large[Again, how do we use it?]]

---
# How do we use it?

Just like Lasso but with `alpha &gt; 0` and `alpha &lt; 1`

```r
## The R Package
library(glmnet)

## Remove Missingness (or impute)
df &lt;- df %&gt;%
  filter(complete.cases(prod1, depr1, marr, chil))

## Outcome
Y &lt;- df$prod2

## Dummy Code
X &lt;- df %&gt;%
  select(-prod2, -nam, -start_date, -last_aired, -show)
X &lt;- model.matrix(~ ., X)[,-1]

fit &lt;- cv.glmnet(X, Y, alpha = 0.5)
plot(fit)
coef(fit, s = "lambda.min")
coef(fit, s = "lambda.1se")
```

---
# How do we use it?

.pull-left[
![](LassoRidge_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

]

.pull-right[

```
##                Variable Lambda.Min Lambda.1SE
## 1           (Intercept)    2.81505    2.17759
## 2                 prod1    1.12212    0.74296
## 3                 ment1   -0.05598          -
## 4                  phys    0.06997          -
## 5                  marr   -0.66176   -0.29321
## 6                  gend   -1.29495   -0.66184
## 7             raceBlack   -0.26051          -
## 8  raceMexican American   -0.06853          -
## 9            raceIndian          -          -
## 10                 inco   -0.03412          -
## 11                 chil     0.3903          -
## 12                 subs    0.32519          -
## 13                 alco   -1.29072   -0.03888
## 14                 spor   -0.13565          -
## 15                depr1    0.06615          -
## 16                awkw1          -          -
## 17                ment2    0.12734          -
## 18                depr2          -          -
## 19                awkw2   -0.02478          -
```
]

---
# Elastic Net

### How do we select the tuning parameters?

With `\(\lambda\)` it is fairly automatic with the `cv.glmnet()` function. But what about `\(\alpha\)`?

1. Base it on theory.
2. Cross-validated error just like with `\(\lambda\)`.
3. Try a few values and see which is lowest.

&lt;br&gt;

None are amazing, but they all are good options.


---
# What if we want unbiased results?

### Use the best model and fit an un-penalized model.

This also gives us standard errors and p-values, which we often want for publications.

Note that this can be problemmatic if there are still a lot of selected variables or the variables are highly correlated.




---
# Other Notable Topics
.large[
1. Generalized Additive Models (GAMs)
1. Tree-based Methods
  1. Random Forests
  1. Ada Boost
  1. Fast-and-Frugal Trees
]


---
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
